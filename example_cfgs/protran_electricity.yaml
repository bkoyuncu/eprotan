log_dir: './logs'
seed: 1
gpu: 1
dataset:
  name: electricity_nips # dataset name
  dim_x: 370             # dimension of the dataset/timeseries
  dim_c: 6               # dim_c = dim_c_lag + dim_c_normal
  dim_c_lag: 3 
  tm: 48                 # total time steps
  tmh: 24                # conditioning time steps
  tmf: 24                # forecasting time steps
  kind: full             # legacy keyword, not used
train:
  batch_size: 64
  ckpt_period: 2000     # for saving the model checkpoint
  share_arch: True      # whether to share the architecture between posterior and conditional prior
model:
  type: protran         # foretran (ours) or protran (baseline)
  dim_z: 16             # dimension of the latent space
  likelihood: laplace   # emission distribution
  reconstruct: True     # whether to use reconstruction loss or not
  epsilon: 0.0000001    # Small constant to avoid numerical errors when normalizing with std = 0
optim:
  max_epoch: 4000          # maximum number of epochs to train
  base_lr: 0.0003          # learning rate
  optimizer: adam          # Currently this keyword is ignored and we always use Adam
  scheduler_gamma: 1.0     # learning rate decay
  scheduler_n_epochs: 1000 # number of epochs after which to decay the learning rate
projection_args:
  dim_list: [376,128]   # Projection args of dim(x) + dim(c) --> dim(h)
embedding_args:         # Positional Embeddings
  arch_name: fixed
  max_len: 48
  mode: MODE_ADD        # Whether to add or concatenate the positional embeddings
  embedding_dim: 128
conditional_prior_args:
  arch_name: 'transformer'                    # for ProTran, we call it 'transformer' per default
  num_heads: 8                                # num. attention heads
  num_layers: 1                               # num. attention layers; 1 (Electricity, Solar), otherwise 2
  dim_list: [128,128,128,16]                  # MLP to generate z^(l)_t from \hat{w}^(l)_t [Equations 8/19 in the ProTran paper] Corresponds to conditional_prior_args --> dim_list in Foretran (E-ProTran); BUT here we have it in every layer not just the last layer
  dim_list_latent_to_w: [16,128,128,128]      # MLP to generate w^(l)_t from z^(l)_t [Equations 9/20 in the ProTran paper]: No corresponding arg. in Foretran (E-ProTran) cfg. as we do not sample in the attention layers
  use_sampling: True                          # whether to use the mean in the CondPriorLayers; no corresponding arg in Foretran (E-ProTran) cfg as we do not sample in the attention layers
encoder_args: # posterior
  dim_list: [128,128,128,16]    # MLP to generate z^(l)_t from \hat{w}^(l)_t AND full history [Equation 11 in the ProTran paper]: Corresponds to encoder_args --> dim_list in Foretran (E-ProTran); BUT here we have it in every layer not just the last layer
decoder_args:
  arch_name: 'MLP'              # MLP, used to predict x_t from z_t
  dim_list: [128,128,128,370] 
  variance: 1.0                 # Scale parameter for the Laplace distribution or the variance for the Gaussian distribution
testing:
  samples: 100                  # number of samples to generate for testing